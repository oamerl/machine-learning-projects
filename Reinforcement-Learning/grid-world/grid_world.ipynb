{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oamerl/machine-learning-projects/blob/main/Reinforcement-Learning/grid-world/grid_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Needed Imports"
      ],
      "metadata": {
        "id": "GG6HtQyNdKnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# the following code ensures that you can see your (print) results for multiple tasks within a coding block\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "PYAS8UiCc3l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Evaluation Algorithm"
      ],
      "metadata": {
        "id": "fDPJTHctdFy_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pABHyNDB7gT3",
        "outputId": "5180cafe-031a-42a0-de73-db28839ec0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value function in case of discount factor gamma =  0.75 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.21211313,  9.3812912 ,  3.34519602,  5.11400177,  0.74879611],\n",
              "       [ 0.65807248,  2.20408747,  1.33343379,  1.24674753,  0.05075425],\n",
              "       [-0.23139657,  0.3822621 ,  0.31572576,  0.1523317 , -0.44037806],\n",
              "       [-0.70940966, -0.25041636, -0.18331346, -0.30985432, -0.77723292],\n",
              "       [-1.25998414, -0.82538043, -0.73503355, -0.84494532, -1.28590421]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Value function in case of discount factor gamma =  0.85 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.91309001,  9.05344334,  4.0499772 ,  5.23293169,  1.18652542],\n",
              "       [ 1.18021666,  2.70545595,  1.89784016,  1.64391587,  0.33127614],\n",
              "       [-0.06964679,  0.59874796,  0.53098714,  0.27390503, -0.42583896],\n",
              "       [-0.861861  , -0.3502687 , -0.27273241, -0.46063032, -1.00715864],\n",
              "       [-1.59910894, -1.11414074, -1.00500664, -1.16323629, -1.67085218]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "-----------------------------------------------------------------------------\n",
            "Value function in case of discount factor gamma =  0.9 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.31283295,  8.79309665,  4.43105838,  5.32555202,  1.49505291],\n",
              "       [ 1.52534712,  2.99596941,  2.25352774,  1.91069663,  0.55032164],\n",
              "       [ 0.05456092,  0.74180281,  0.67654928,  0.36141026, -0.40006032],\n",
              "       [-0.96980851, -0.43179294, -0.35132735, -0.58220167, -1.17977415],\n",
              "       [-1.85386334, -1.34146119, -1.22560997, -1.41938165, -1.97171949]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "-----------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "grid_cols = 5 # number of columns in grid (x-axis)\n",
        "grid_rows = 5 # number of rows in grid (y-axis)\n",
        "\n",
        "discount_factors = [0.75, 0.85, 0.9] # discount factors to be tried\n",
        "theta_thres = 0.01 # theta threshold for accuracy\n",
        "\n",
        "# Define deterministic state transition probability\n",
        "state_transition_prob = 1\n",
        "\n",
        "# uniform policy with probability of 0.25 for each action to be chosen\n",
        "actions_prob_policy = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "# list of possible actions defined as numpy arrays to be able to apply mathematical operations on them\n",
        "actions = [np.array([-1, 0]), # North\n",
        "           np.array([1, 0]), # South\n",
        "           np.array([0, -1]), # West\n",
        "           np.array([0, 1])] # East\n",
        "\n",
        "cell_A = [0, 1] ; cell_A_prime = [4, 1] # unique cell A location\n",
        "cell_B = [0, 3] ; cell_B_prime = [2, 3] # unique cell B location\n",
        "\n",
        "# initial value function initialized with zeros\n",
        "previous_value = np.zeros((grid_cols, grid_rows))\n",
        "\n",
        "# loop over different discount factors and getting value function for each one\n",
        "for dicount_factor in discount_factors:\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # initialization for current iteration value\n",
        "        current_iter_value =  np.zeros((grid_cols, grid_rows))\n",
        "\n",
        "        # evaluate the value function for each state\n",
        "        for y in range(grid_rows):\n",
        "            for x in range(grid_cols):\n",
        "\n",
        "                current_state = [y,x] # current state location\n",
        "\n",
        "                for action_indx in range(len(actions)): # looping over possible actions for each state\n",
        "\n",
        "                    # Unique Cell A state condition\n",
        "                    if current_state == cell_A:\n",
        "                        next_state = cell_A_prime # unique condtion of teleporting to state A prime\n",
        "                        reward = 10.0 # special reward value to be accrued\n",
        "\n",
        "                    # Unique Cell B state condition\n",
        "                    elif current_state == cell_B:\n",
        "                        next_state = cell_B_prime # unique condtion of teleporting to state B prime\n",
        "                        reward = 5.0 # special reward value to be accrued\n",
        "\n",
        "                    # For any other state other than A or B\n",
        "                    else:\n",
        "                        next_state = np.array(current_state) + actions[action_indx] # the next state to land on based on the action\n",
        "\n",
        "                        # Check if next state is outside the grid\n",
        "                        if next_state[0] < 0 or next_state[0] >= grid_rows or next_state[1] < 0 or next_state[1] >= grid_cols:\n",
        "                            next_state = current_state\n",
        "                            reward = -1.0\n",
        "                        # if next state is within the grid\n",
        "                        else:\n",
        "                            reward = 0.0\n",
        "\n",
        "                    action_prob = actions_prob_policy[action_indx] # getting the probability of current action\n",
        "\n",
        "                    # new value\n",
        "                    current_iter_value[y, x] = current_iter_value[y, x] + action_prob * (state_transition_prob *(reward + dicount_factor * previous_value[next_state[0], next_state[1]]))\n",
        "\n",
        "        # if the difference between the values is below the accuracy paramter theta will break and return current_iter_value\n",
        "        if np.sum(np.abs(previous_value - current_iter_value)) < theta_thres:\n",
        "            break\n",
        "        # couldn't implement the idea of delta so used the above condition instead, another way would be going around the whole grid for a defined number of times instead of uisng while loop\n",
        "\n",
        "        # if the condition is not satisfied will go for another interation\n",
        "        else:\n",
        "            previous_value = current_iter_value # updating the previous iteration variable with the newly calculated value\n",
        "\n",
        "    print(\"Value function in case of discount factor gamma = \", dicount_factor, \"\\n\")\n",
        "    current_iter_value\n",
        "    print(\"\\n\")\n",
        "    print(\"-----------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cond = np.abs(previous_value - current_iter_value) < theta_thres\n",
        "#if False not in cond:\n",
        "#    break\n",
        "\n",
        "#delta = np.maximum(delta, np.abs(previous_value - current_iter_value))"
      ],
      "metadata": {
        "id": "zlHg27Sod-o6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}